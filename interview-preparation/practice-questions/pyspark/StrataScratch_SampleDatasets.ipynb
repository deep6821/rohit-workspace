{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, count, desc, lit, max, rank, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_schema = StructType([\n",
    "    StructField(\"worker_id\", IntegerType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"joining_date\", DateType(), True),\n",
    "    StructField(\"department\", StringType(), True)\n",
    "])\n",
    "\n",
    "worker_data = [\n",
    "    (1, \"Monika\", \"Arora\", 100000, datetime.date(2014, 2, 20), \"HR\"),\n",
    "    (2, \"Niharika\", \"Verma\", 80000, datetime.date(2014, 6, 11), \"Admin\"),\n",
    "    (3, \"Vishal\", \"Singhal\", 300000, datetime.date(2014, 2, 20), \"HR\"),\n",
    "    (4, \"Amitah\", \"Singh\", 500000, datetime.date(2014, 2, 20), \"Admin\"),\n",
    "    (5, \"Vivek\", \"Bhati\", 500000, datetime.date(2014, 6, 11), \"Admin\"),\n",
    "    (6, \"Vipul\", \"Diwan\", 200000, datetime.date(2014, 6, 11), \"Account\"),\n",
    "    (7, \"Satish\", \"Kumar\", 75000, datetime.date(2014, 1, 20), \"Account\"),\n",
    "    (8, \"Geetika\", \"Chauhan\", 90000, datetime.date(2014, 4, 11), \"Admin\"),\n",
    "    (9, \"Agepi\", \"Argon\", 90000, datetime.date(2015, 4, 10), \"Admin\"),\n",
    "    (10, \"Moe\", \"Acharya\", 65000, datetime.date(2015, 4, 11), \"HR\"),\n",
    "    (11, \"Nayah\", \"Laghari\", 75000, datetime.date(2014, 3, 20), \"Account\"),\n",
    "    (12, \"Jai\", \"Patel\", 85000, datetime.date(2014, 3, 21), \"HR\")\n",
    "]\n",
    "\n",
    "worker_columns = [\"worker_id\", \"first_name\", \"last_name\", \"salary\", \"joining_date\", \"department\"]\n",
    "worker_df = spark.createDataFrame(data=worker_data, schema=worker_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Full Name and Highest Salary\n",
    "worker_df = worker_df.withColumn(\"full_name\", concat(worker_df.first_name, lit(\" \"), worker_df.last_name))\n",
    "max_salary = worker_df.agg({\"salary\": \"max\"}).collect()[0][0]\n",
    "worker_df.filter(worker_df.salary == max_salary).select(\"full_name\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2:  Highest salary in each department\n",
    "\n",
    "# max_salary_df = worker_df.groupBy(\"department\").agg(max(\"salary\").alias(\"max_salary\"))\n",
    "# max_salary_df.join(worker_df.alias(\"w\"), (max_salary_df.department == col(\"w.department\")) & (max_salary_df.max_salary == col(\"w.salary\")), \"inner\").select(\"w.department\", \"w.full_name\", \"w.salary\").show()\n",
    "\n",
    "# OR\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(desc(\"salary\"))\n",
    "ranked_df = worker_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "ranked_df.filter(col(\"rank\") == 1).select(\"department\", \"full_name\", \"salary\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3: Last Five Records of Dataset\n",
    "# worker_df.show(5)\n",
    "worker_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_campaign_data = [\n",
    "    ('10', '2019-01-01', '101', '3', '55'),\n",
    "    ('10', '2019-01-02', '119', '5', '29'),\n",
    "    ('10', '2019-03-31', '111', '2', '149'),\n",
    "    ('11', '2019-01-02', '105', '3', '234'),\n",
    "    ('11', '2019-03-31', '120', '3', '99'),\n",
    "    ('12', '2019-01-02', '112', '2', '200'),\n",
    "    ('12', '2019-03-31', '110', '2', '299'),\n",
    "    ('13', '2019-01-05', '113', '1', '67'),\n",
    "    ('13', '2019-03-31', '118', '3', '35'),\n",
    "    ('14', '2019-01-06', '109', '5', '199'),\n",
    "    ('14', '2019-01-06', '107', '2', '27'),\n",
    "    ('14', '2019-03-31', '112', '3', '200'),\n",
    "    ('15', '2019-01-08', '105', '4', '234'),\n",
    "    ('15', '2019-01-09', '110', '4', '299'),\n",
    "    ('15', '2019-03-31', '116', '2', '499'),\n",
    "    ('16', '2019-01-10', '113', '2', '67'),\n",
    "    ('16', '2019-03-31', '107', '4', '27'),\n",
    "    ('17', '2019-01-11', '116', '2', '499'),\n",
    "    ('17', '2019-03-31', '104', '1', '154'),\n",
    "    ('18', '2019-01-12', '114', '2', '248'),\n",
    "    ('18', '2019-01-12', '113', '4', '67'),\n",
    "    ('19', '2019-01-12', '114', '3', '248'),\n",
    "    ('20', '2019-01-15', '117', '2', '999'),\n",
    "    ('21', '2019-01-16', '105', '3', '234'),\n",
    "    ('21', '2019-01-17', '114', '4', '248'),\n",
    "    ('22', '2019-01-18', '113', '3', '67'),\n",
    "    ('22', '2019-01-19', '118', '4', '35'),\n",
    "    ('23', '2019-01-20', '119', '3', '29'),\n",
    "    ('24', '2019-01-21', '114', '2', '248'),\n",
    "    ('25', '2019-01-22', '114', '2', '248'),\n",
    "    ('25', '2019-01-22', '115', '2', '72'),\n",
    "    ('25', '2019-01-24', '114', '5', '248'),\n",
    "    ('25', '2019-01-27', '115', '1', '72'),\n",
    "    ('26', '2019-01-25', '115', '1', '72'),\n",
    "    ('27', '2019-01-26', '104', '3', '154'),\n",
    "    ('28', '2019-01-27', '101', '4', '55'),\n",
    "    ('29', '2019-01-27', '111', '3', '149'),\n",
    "    ('30', '2019-01-29', '111', '1', '149'),\n",
    "    ('31', '2019-01-30', '104', '3', '154'),\n",
    "    ('32', '2019-01-31', '117', '1', '999'),\n",
    "    ('33', '2019-01-31', '117', '2', '999'),\n",
    "    ('34', '2019-01-31', '110', '3', '299'),\n",
    "    ('35', '2019-02-03', '117', '2', '999'),\n",
    "    ('36', '2019-02-04', '102', '4', '82'),\n",
    "    ('37', '2019-02-05', '102', '2', '82'),\n",
    "    ('38', '2019-02-06', '113', '2', '67'),\n",
    "    ('39', '2019-02-07', '120', '4', '99')\n",
    "]\n",
    "\n",
    "marketing_campaign_columns = [\"user_id\", \"created_at\", \"product_id\", \"quantity\", \"price\"]\n",
    "marketing_campaign_df = spark.createDataFrame(marketing_campaign_data, marketing_campaign_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert quantity to integer type\n",
    "marketing_campaign_df = marketing_campaign_df.withColumn(\"quantity\", col(\"quantity\").cast(\"int\"))\n",
    "# Calculate total units sold for each product\n",
    "product_units_sold_df = marketing_campaign_df.groupBy(\"product_id\").agg({\"quantity\": \"sum\"}).withColumnRenamed(\"sum(quantity)\", \"total_units_sold\")\n",
    "# Categorize ad performance\n",
    "product_units_sold_df = product_units_sold_df.withColumn(\n",
    "    \"ad_performance\", \n",
    "    when(col(\"total_units_sold\") >= 30, \"Outstanding\")\n",
    "    .when((col(\"total_units_sold\") >= 20) & (col(\"total_units_sold\") <= 29), \"Satisfactory\")\n",
    "    .when((col(\"total_units_sold\") >= 10) & (col(\"total_units_sold\") <= 19), \"Unsatisfactory\")\n",
    "    .when((col(\"total_units_sold\") >= 1) & (col(\"total_units_sold\") <= 9), \"Poor\")\n",
    "    .otherwise(\"Unknown\")\n",
    ")\n",
    "# Sort by total units sold in descending order\n",
    "product_units_sold_df = product_units_sold_df.orderBy(col(\"total_units_sold\").desc())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
