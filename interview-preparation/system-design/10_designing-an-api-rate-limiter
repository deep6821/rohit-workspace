Let's design an API Rate Limiter which will throttle users based upon the number of the requests they are
sending.

1. What is a Rate Limiter?
2. Why do we need API rate limiting?
3. Requirements and Goals of the System
4. How to do Rate Limiting?
5. What are different types of throttling?
6. What are different types of algorithms used for Rate Limiting?
7. High level design for Rate Limiter
8. Basic System Design and Algorithm
9. Sliding Window algorithm
10. Sliding Window with Counters
11. Data Sharding and Caching
12. Should we rate limit by IP or by user?

1. What is a Rate Limiter?
--------------------------
Imagine we have a service which is receiving a huge number of requests, but it can only serve a limited number
of requests per second. To handle this problem we would need some kind of throttling or rate limiting mechanism
that would allow only a certain number of requests so our service can respond to all of them. A rate limiter, at
a high-level, limits the number of events an entity (user, device, IP, etc.) can perform in a particular time
window. For example:

- A user can send only one message per second
- A user is allowed only three failed credit card transactions per day.
- A single IP can only create twenty accounts per day.

In general, a rate limiter caps how many requests a sender can issue in a specific time window. It then blocks
requests once the cap is reached.

2. Why do we need API rate limiting?
------------------------------------
Rate Limiting helps to protect services against abusive behaviors targeting the application layer like
Denial-of-service (DOS) attacks, brute-force password attempts, brute-force credit card transactions, etc.
These attacks are usually a barrage of HTTP/S requests which may look like they are coming from real  users,
but are typically generated by machines (or bots). As a result, these attacks are often harder to detect and
can more easily bring down a service, application, or an API.

Rate limiting is also used to prevent revenue loss, to reduce infrastructure costs, to stop spam, and to stop
online harassment. Following is a list of scenarios that can benefit from Rate limiting by making a
service (or API) more reliable:

- Misbehaving clients/scripts: Either intentionally or unintentionally, some entities can overwhelm a service
by sending a large number of requests. Another scenario could be when a user is sending a lot of lower-priority
requests and we want to make sure that it doesn’t affect the high-priority traffic. For example, users sending a
high volume of requests for analytics data should not be allowed to hamper critical transactions for other users.

- Security: By limiting the number of the second-factor attempts (in 2-factor auth) that the users are allowed
to perform, for example, the number of times they’re allowed to try with a wrong password.

- To prevent abusive behavior and bad design practices: Without API limits, developers of client applications
would use sloppy development tactics, for example, requesting the same information over and over again.

- To keep costs and resource usage under control: Services are generally designed for normal input behavior,
for example, a user writing a single post in a minute. Computers could easily push thousands/second through an
API. Rate limiter enables controls on service APIs.

- Revenue: Certain services might want to limit operations based on the tier of their customer’s service and
thus create a revenue model based on rate limiting. There could be default limits for all the APIs a service
offers. To go beyond that, the user has to buy higher limits

- To eliminate spikiness in traffic: Make sure the service stays up for everyone else.

3. Requirements and Goals of the System
---------------------------------------
Our Rate Limiter should meet the following requirements:
Functional Requirements:
a. Limit the number of requests an entity can send to an API within a time window, e.g., 15 requests per second.
b. The APIs are accessible through a cluster, so the rate limit should be considered across different servers.
The user should get an error message whenever the defined threshold is crossed within a single server or across
a combination of servers.

Non-Functional Requirements:
a. The system should be highly available. The rate limiter should always work since it protects our service from
external attacks.
b. Our rate limiter should not introduce substantial latencies affecting the user experience.

4. How to do Rate Limiting?
---------------------------
a. Rate Limiting is a process that is used to define the rate and speed at which consumers can access APIs.
b. Throttling is the process of controlling the usage of the APIs by customers during a given period.
Throttling can be defined at the application level and/or API level. When a throttle limit is crossed, the
server returns HTTP status “429 - Too many requests".

5. What are different types of throttling?
------------------------------------------
Here are the three famous throttling types that are used by different services:

a) Hard Throttling: The number of API requests cannot exceed the throttle limit.
b) Soft Throttling: In this type, we can set the API request limit to exceed a certain percentage. For example,
if we have rate-limit of 100 messages a minute and 10% exceed-limit, our rate limiter will allow up to 110
messages per minute.
c) Elastic or Dynamic Throttling: Under Elastic throttling, the number of requests can go beyond the threshold
if the system has some resources available. For example, if a user is allowed only 100 messages a minute, we
can let the user send more than 100 messages a minute when there are free resources available in the system.

6. What are different types of algorithms used for Rate Limiting?
-----------------------------------------------------------------
Following are the two types of algorithms used for Rate Limiting:

a) Fixed Window Algorithm: In this algorithm, the time window is considered from the start of the time-unit to
the end of the time-unit. For example, a period would be considered 0-60 seconds for a minute irrespective of
the time frame at which the API request has been made. In the diagram below, there are two messages between
0-1 second and three messages between 1-2 seconds. If we have a rate limiting of two messages a second, this
algorithm will throttle only ‘m5’.

b) Rolling Window Algorithm: In this algorithm, the time window is considered from the fraction of the time at
which the request is made plus the time window length. For example, if there are two messages sent at the
300th millisecond and 400th millisecond of a second, we’ll count them as two messages from the 300th
millisecond of that second up to the 300th millisecond of next second. In the above diagram, keeping two
messages a second, we’ll throttle ‘m3’ and ‘m4’.

7. High level design for Rate Limiter
-------------------------------------
Rate Limiter will be responsible for deciding which request will be served by the API servers and which request
will be declined. Once a new request arrives, the Web Server first asks the Rate Limiter to decide if it will
be served or throttled. If the request is not throttled, then it’ll be passed to the API servers.

8. Basic System Design and Algorithm:
-------------------------------------
Let’s take the example where we want to limit the number of requests per user. Under this scenario,
for each unique user, we would keep a count representing how many requests the user has made and a
timestamp when we started counting the requests. We can keep it in a hashtable,
where the ‘key’ would be the ‘UserID’ and ‘value’ would be a list(array) containing an integer for the
‘Count’ and 'timestamp':

Ex: {'key': [value]} -->> {'Rohit': [count, timestamp]}

Let’s assume our rate limiter is allowing three requests per minute per user, so whenever a new request
comes in, our rate limiter will perform the following steps:

1. If the ‘UserID’ is not present in the hash-table, insert it, set the ‘Count’ to 1, set ‘StartTime’ to the current
time (normalized to a minute), and allow the request.

2. Otherwise, find the record of the ‘UserID’ and if CurrentTime – StartTime >= 1 min, set the ‘StartTime’ to the
current time, ‘Count’ to 1, and allow the request.

3. If CurrentTime - StartTime <= 1 min and
a) If ‘Count < 3’, increment the Count and allow the request.
b) If ‘Count >= 3’, reject the request.

What are some of the problems with our algorithm?
-------------------------------------------------
1. This is a Fixed Window algorithm since we’re resetting the ‘StartTime’ at the end of every minute,
which means it can potentially allow twice the number of requests per minute. Imagine if Rohit sends
three requests at the last second of a minute, then she can immediately send three more requests at the
very first second of the next minute, resulting in 6 requests in the span of two seconds.
The solution to this problem would be a sliding window algorithm which we’ll discuss later.

Atomicity: In a distributed environment, the “read-and-then-write” behavior can create a race condition.
Imagine if Rohit’s current ‘Count’ is “2” and that she issues two more requests. If two separate
processes served each of these requests and concurrently read the Count before either of them  updated it,
each process would think that Kristie could have one more request and that she had not hit the rate limit.