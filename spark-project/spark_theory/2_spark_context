SparkContext is available since Spark 1.x (JavaSparkContext for Java) and is used to be an entry point to Spark and
PySpark before introducing SparkSession in 2.0. Creating SparkContext was the first step to program with RDD and to
connect to Spark Cluster.

What is SparkContext?
---------------------
Since Spark 1.x, Spark SparkContext is an entry point to Spark and defined in org.apache.spark package and used to
programmatically create Spark RDD, accumulators, and broadcast variables on the cluster. Its object sc is default
variable available in spark-shell and it can be programmatically created using SparkContext class.

Most of the operations/methods or functions we use in Spark come from SparkContext for example accumulators, broadcast
variables, parallelize, and more.

Note that you can create only one SparkContext per JVM. At any given time only one SparkContext instance should be
active per JVM. In case if you want to create a another new SparkContext you should stop existing SparkContext
(using stop()) before creating a new one.

=======================================================================================================================
SparkContext commonly used methods:
-----------------------------------
accumulator – It creates an accumulator variable of a given data type. Only a driver can access accumulator variables.

applicationId – Returns a unique ID of a Spark application.

appName – Return an app name that was given when creating SparkContext

broadcast – read-only variable broadcast to the entire cluster. You can broadcast a variable to a Spark cluster only
once.

emptyRDD – Creates an empty RDD

getPersistentRDDs – Returns all persisted RDD’s

getOrCreate – Creates or returns a SparkContext

hadoopFile – Returns an RDD of a Hadoop file

master()–  Returns master that set while creating SparkContext

newAPIHadoopFile – Creates an RDD for a Hadoop file with a new API InputFormat.

sequenceFile – Get an RDD for a Hadoop SequenceFile with given key and value types.

setLogLevel – Change log level to debug, info, warn, fatal and error

textFile – Reads a text file from HDFS, local or any Hadoop supported file systems and returns an RDD

union – Union two RDD’s

wholeTextFiles – Reads a text file in the folder from HDFS, local or any Hadoop supported file systems and returns an
RDD of Tuple2. First element of the tuple consists file name and the second element consists context of the text file.
