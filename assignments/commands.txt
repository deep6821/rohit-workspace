---------------------------------------------------------------
# Use the official Apache Spark PySpark image as a parent image
FROM apache/spark-py:v3.3.0

# Set the working directory in the container
WORKDIR /app

# Copy your PySpark script and any other files required
COPY main.py /app
COPY data.csv /app

# Set the environment variable to point to the Spark bin directory
ENV PATH="${SPARK_HOME}/bin:${PATH}"

# When you start the container, it will automatically execute the Spark job specified in main.py
# CMD ["spark-submit", "--master", "local[*]", "main.py"]

# Run a long-lived process in the foreground to keep the container running
# CMD ["tail", "-f", "/dev/null"]

# When you start the container, it will open a shell prompt and then, you can manually run the Spark job inside the container by: spark-submit main.py
CMD ["/bin/bash"]


---------------------------------------------------------------

docker build -t hcl-pyspark-app .

docker images

docker run -it hcl-pyspark-app:latest

# docker run --privileged -it rohit-pyspark-app:latest
--------------------------------

docker ps -a
docker start 15debeb49ec2
docker exec -it 15debeb49ec2 /bin/bash    # docker exec -it <container_id> /bin/bash


---------------------------------------------------------------


python: 3.9.2
pyspark version: 3.3.0


https://www.youtube.com/watch?v=9gtOkz4Ybdg

https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/
https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.9.9/
